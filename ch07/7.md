# 七、The VC Dimension VC维

## 7.1 Definition of VC Dimension VC维的定义

先对上一章的内容作简单总结：如果一个假设空间存在突破点。则一定存在成长函数$m_H(N)$被某个上限函数$B(N,k)$所约束，可求出上限函数等于一个组合的求和形式$\sum_{i=0}^{k-1}C_N^i$，易知该形式的最高次项是$N^{k-1}$。图7-1a)和b）分别是以上限函数为成长函数上限的情况和以为成长函数上限的情况。

<div align='center'><img src='http://i4.bvimg.com/602813/c86d080e03347f3b.png'>
<img src='http://i1.bvimg.com/602813/e133bb9e2e1ce171.png'></div>
<center>图7-1 a) 以上限函数为上限 b) 以$N^{k-1}$为上限
 </center>
 从图中可以看出在$N \ge 2$ 且 $K \ge 3$的情况下，满足$B(N,k) \le N{K-1}$，得到公式7-1。
 $$
 m_H(N) = B(N,k) = \sum_{i=0}^{k-1}C_N^i \le N^{k-1}
 $$

 通过公式7-1和上一章的结论可以得出公式7-2。
 $$
 \begin{aligned}
 P_D[|E_{in}(g) - E_{out}(g)| > \varepsilon]
&\le P_D[\exists h \in s.t. |E_{in} - E_{out}| > \varepsilon] \\
&\le 4m_H(2N)\exp(-\frac{1}{8}\varepsilon^2N) \\
&\le^{\text{if K exists}} 4N^{k-1}\exp(-\frac{1}{8}\varepsilon^2N)
 \end{aligned}
 $$

该公式的意义是在输入样本N很大时，VC限制一定成立，同时等式的左边也一定会在$k \ge 3$ 的情况下被以多项式形式（$N^{k-1}$）所约束（注意这里$N \ge 2$的条件没有了，原因很简单，VC限制是样本N很大的情况下产生的，因此一定满足$N \ge 2$的条件），而在k<3的情况下有其他的限制可以满足（比如前几章提到的如正射线之类的分类不需要多项式形式的限制也可以约束住成长函数）。

至此得知，满足以下几个条件，机器便可以学习：

1. 假设空间的成长函数有一个突破点k（有好的假设空间H）；
2. 输入数据样本N足够的大（有好的输入样本集D）；

1和2通过VC限制共同推出了$E_{in}$和$E_{out}$有很大的可能很接近。

1. 一个算法A能够找出一个使 $E_{in}$ 足够小的g（好的算法A）；

再结合1和2得出的结论就可以进行学习了（当然这里需要一点好的运气）。

接下来介绍一下这一节的正题，VC维度或者VC维（VC dimension）是什么意思。

它的定义和突破点（break point）有很大关系，是最大的一个不是突破点的数。

VC维是假设空间的一个性质，数据样本可以被完全二分的最大值。用$d_{VC}$作为VC维的数学符号，假如突破点存在的话，即最小的突破点减去1，如公式7-3所示；如果不存在突破点的话，则VC维为无限大。
$$d_{VC} = '\text{最小的}k' -1$$

如果输入数据量N小于VC维$d_{VC}$，则有可能输入数据D会被完全的二分类，这里不是一定，只能保证存在。

如果输入数据量N（或者用k表示）大于VC维$d_{VC}$，则有k一定是假设空间H的突破点。

使用VC维$d_{VC}$对公式7-1进行重写，在$N \ge 2$ 且$ d_{VC} ge 2$时，如公式7-4所示。
$$
m_H(N) \le N^{d_{VC}}
$$
对第五章中提到的几种分类，使用VC维取代突破点，表示VC维与成长函数的关系，如表7-1所示。
<div align='center'><img src='http://i2.bvimg.com/602813/04abb29b024411b1.png'></div>

对上述条件1中好的假设空间重新做一个定义，**即有限的VC维$d_{VC}$**。

一个有限的VC维总是能够保证寻找到的近似假设g满足$E_{in}(g) \approx E_{out}(g)$，这一结论与下述部分没有关系：

1. 使用的算法A，即使很大$E_{in}(g)$，也依然能满足上述的性质；
2. 输入数据的分布P；
3. 未知的目标函数f。

即VC维可应对任意的假设空间，任意的数据分布情况，任意的目标函数。

满足这一性质可以得到如图7-2所示的流程图，其中灰色的部分表示上述几个不会影响$E_{in}(g) \approx E_{out}(g)$这一结果的部分。
<div align='center'><img src='http://i1.bvimg.com/602813/b3100a7ebd857ceb.png'></div>
<center>图7-2 由VC维保证机器可以学习的流程图</center>

## 7.2 VC Dimension of Perceptrons 感知器的VC纬

以下两个条件保证了2维线性可分的数据是可以学习的。

1. 线性可分的数据通过PLA算法运行足够长的时间（T步骤足够大），则会找出一条可以正确分类的直线，使得样本中没有产生分错类的情况，即$E_{in}(g) = 0$；
2. 在训练样本和整个数据集都服从同一分布P的前提下，有VC限制保证了，在且训练样本N足够大时，$E_{in}(g) \approx E_{out}(g)$。

以上两个条件共同得出$E_out(g) \approx 0$的结论。

这一节讨论的是PLA能否处理维数大于二维的数据。

从上一节的内容得知：只要求出$d_{VC}$是一个有限数，则可以使用VC限制来保证$E_{in}(g) \approx E_{out}(g)$。于是问题变成了在维数大于二维时，感知器的VC维$d_{VC}$如何表示（能否表示成一个有限数）。

两种已知感知器的VC维表示。1维感知器的VC维：$d_{VC} =2$；2维感知器的VC维：$d_{VC} = 3$。

能否以此类推得出d维感知器的VC维：$d_{VC} = d + 1$呢？

上述只是一种猜想，接下来是对此猜想进行证明，证明的思路也很传统，证明等于号的成立分为两步：证明大于等于$d_{VC}$以及小于等于$d_{VC} \le d+1$。

证明大于等于的思路：证明存在d+1数量的某一数据集可以完全二分；证明小于等于的思路：证明任何d+2数量的数据集都不可以完全二分。

首先证明大于等于。因为只需要证明存在，不妨构造一个输入样本集，假设样本为一个行向量，其中第一个样本为0向量，第二个样本是其第一个分量为1其他分量为0的向量，第三个样本是其第二个分量为1其他分量为0的向量，以此类推，第d+1个样本是其第d个分量为1其他分量为0的向量，如： ，，，…，，在感知器中样本X如公式7-5所示，其中每一个样本加上默认的第0个分量，其值为1（从阈值b变成所乘的样本分量）。