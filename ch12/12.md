# 十二、Nonlinear Transformation  非线性转换

## 12.1 Quadratic Hypotheses   二次的假设空间

在之前的章节中，学习的机器学习模型都为线性模型，即假设空间都为线性的，使用的得分为线性得分（linear scores）$s = w^Tx$ 。这种线性模型的最大好处是理论上可以使用VC限制进行约束（假设是非线性的各种曲线或者超曲面，则每种假设都可以完全二分，即 $2^N$种二分类），这保证了 $E_{in} = E_{out}$；当遇到如图12-1的情况，则很难寻找到一个直线能将两类尽可能的分离，即存在很大的 $E_{in}$。该如何打破这种限制？

![Markdown](http://i2.bvimg.com/602813/1fb691f8defbcc70.png)

图12-1 线性不可分的情况

观察图12-1发现该数据集为线性不可分（non-linear separable）的情况，可以使用圆圈将样本集分离，如图12-2所示，此种方式称作圆圈可分（circular separable），该图使用一个半径为$\sqrt{0.6}$ 圆心在原点的圆划分，假设函数h如公式12-2所示，该公式的含义为将样本点到原点距离的平方与数值0.6作比较，如果比0.6小，则标记为+1；反之为-1。

![Markdown](http://i1.bvimg.com/602813/f1855ece82a0f38e.png)

图12-2 圆圈可分的情况

$$h_{SEP}(x) = sign(-x_1^2 - x_2^2 + 0.6)$$

难道需要再将以前学过的线性PLA，线性回归的算法都重新设计一遍，变成圆圈PLA，圆圈回归重新学习？当然不会这样，以下介绍一种思想通过已有知识解决上述新提出的算法模型。

将公式12-1中变量以及参数做一些转变，变成熟悉的线性模型，如公式12-2所示。

$$h_{SEP}(x) = sign(\underbrace{0.6}_{w_0} \times \underbrace{1}_{x_0} + \underbrace{(-1)}_{w_1} \times \underbrace{x_1^2}_{x_1} + \underbrace{(-1)}_{w_2} \times \underbrace{x_2^2}_{x_2}) = sign(w_0 \cdot z_0 + w_1 \cdot z_1 + w_2 \cdot z_2) = sign(w^Tz)$$

该公式将圆圈可分$\{ (x_n, y_n)\}$ 转换成线性可分$\{(z_n,y_n)\}$，得到的结果如图12-3所示。称这种将输入空间$x \in X \rightarrow^{\Theta} \text{输出空间} z\in Z$ 的过程称为特征转换（feature transform $\Theta$）。

![Markdown](http://i1.bvimg.com/602813/fabab0f3eda9165f.png)

图12-3 在Z空间线性可分的情况

问题出现了，是否新的空间中数据线性可分，则在原空间中原数据一定是圆圈可分？搞清此问题之前，需要了解在新的空间中如何线性可分的。

新的空间Z的表示如公式12-3所示。

$$(z_0, z_1, z_2) = z = \Phi(x) = (1, x_1^2, x_2^2) $$

在空间X中假设函数h与空间Z假设函数$h$ 的关系如公式12-4所示。

$$h(x) = \widetilde h(z) = sign(\widetilde w_0 + \widetilde w_1x_1^2 + \widetilde w_2x_2^2)$$

公式12-4即为在X空间中假设函数的表达式，其中$(\widetilde w_0, \widetilde w_1, \widetilde w_2)$为权值向量，观察权值向量的不同取值对X空间的中的假设函数的表达式有何不同，如表12-1所示。

![Markdown](http://i2.bvimg.com/602813/5a20f08c6f4b2595.png)

因此通过这种形式转换的Z空间的直线对应着原X空间中特殊的二次曲线（quadratic curves）。为何说是特殊的？从表12-1第一行表示的圆只能是圆心过原点的圆，不能随意表示各种情况的圆。

如果想要表示X空间中所有的二次曲面，Z空间该佮表示呢？设计一个更大的Z空间，其特征转换如公式12-5所示。

$$\Phi _2(x) = (1, x_1, x_2, x_1^2, x_1x_2, x_2^2)$$

通过以上特征转换，Z空间中的每个超平面就对应X空间中各种不同情况的二次曲线。则X空间中的假设空间H如公式12-6所示。

$$H_{\Phi_2} = \{h(x):h(x) = \widetilde h(\Phi_2(x)) \}$$

其中$\widetilde h$表示Z空间中的假设函数。使用一个例子表示，如公式12-7为斜椭圆的表达式，可以得出权值向量 $\widetilde w^T = [33, -20, -4, 3, 2, 3]$。

$$2(x_1 + x_2 - 3)^2 + (x_1 - x_2 - 4)^2 = 1 \qquad \text{公式12-7}$$

使用公式12-5的特征转换，可以表示X空间中所有的线（包括直线和各种类型的二次曲线）和常数（全为正或全为负）。

## 12.2 Nonlinear Transform    非线性转换

从X空间转换到Z空间，则在Z空间中获得的好的线性假设函数，相当于在X空间中获得了好的二次假设函数，即在Z空间中存在一个可分的直线对应于在X空间存在一个可分的二次曲线。如何在Z空间中寻找好的假设函数呢？

将在Z空间的数据集写成如$\{(z_n = \Phi(x_n), y_n )\}$ ，前面的章节讲述了如何在X空间的数据集中$\{(x_n, y_n)\}$寻找最好的假设函数，因此可以使用前面章节学习到的方法对Z空间的数据集进行训练。

简述下此种学习方式的步骤，转换与学习步骤如图12-4所示：

通过特征转换函数$\Phi$ 将在X空间中不可分的数据集$\{(x_n, y_n)\}$转换成在Z空间中可分的数据集$\{(z_n = \Phi(x_n), y_n )\}$；

