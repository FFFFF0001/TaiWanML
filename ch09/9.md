# 九、Linear Regression 线性回归

## 9.1 Linear Regression Problem 线型回归问题

在第二章中提到的银行发放信用卡问题，通过是否发放信用卡引出了二元分类问题；本章再次使用这个例子通过发放用户多大额度的信用卡引出回归（regression）或者说线性回归（linear regression）的问题。回归问题与二元分类问题最大的不同在于输出空间，二元分类的输出空间为二元标记，要么+1要么-1，而回归问题的输出空间是整个实数空间，即$ y \in R $  。

以银行发放信用卡为例，输入集合依然是用户的特征空间，如年龄，收入等等，可以使用与二元分类一致的表示方式$x^T = [X_0, X_1, \cdots, X_d] $；因为输出集合的转变导致回归问题的假设函数与二元分类中的有所不同，但思想一致，仍需考虑对每个输入样本的各个分量进行加权求和，因此最终目标函数f（含有噪音，使用y表示）的表示如公式9-1所示。

$$ y \approx \sum_{i=0}^dW_iX_i $$ 

而假设函数的向量表示如公式9-2所示：

$$ h(x) = w^TX  \qquad \text{公式9-2}$$  

从公式9-2的表示可以方便看书，与二元分类假设函数的表示只差了一个取正负号的函数sign。

使用图像的方式更形象的描述线性回归，如图9-1所示。

![Markdown](http://i4.bvimg.com/602813/1c486fcc10c5c5a7.png)

<center>图9-1 a) 1维输入空间的线性回归 b) 2维空间的线性回归</center>

图9-1a中表示输入空间为1维的线性回归表示，其中圆圈○表示输入样本点，蓝色直线表示假设函数$h(x) = W^TX$，连接圆圈与蓝色直线之间的红色线段表示样本点到假设函数的距离，称为剩余误差（residuals），在9-1b中有类似表示。而设计算法的核心思想是使**总体剩余误差最小**。

上一章中也提到过回归使用的错误衡量是平方误差$err(\vec y , y) = (\vec y, y)^2$，因此$E_{in}(h)$如公式9-3所示。

$$E_{in}(h) = \frac{1}{N} \sum^N_{n=1}(h(x_n) - y_n)^2 \qquad \text{公式9-3}$$

同理，$E_{out}(W)$表示公式9-5所示，注意这里使用的是含有噪音的形式，因此$(x,y)$服从联合概率分布P。

$$ E_{out}(W) = E_{(x,y) \sim P}(W^Tx_n - y_n)^2 \qquad \text{公式9-5}$$

VC限制可以约束各种情况的学习模型，当然回归类型的模型也被也受此约束，想要学习到知识，只需要寻找$E_{in}(W)$足够小可以满足$E_{out}(W)$足够小的需求。



## 9.2 Linear Regression Algorithm 线性回归算法

此节重点是如何寻找最小的$E_{in}(W)$，为了表达的方便，将求和公式转换成向量与矩阵的形式，将公式9-4转换成公式9-6的形式。$$E_{in}(W) = \frac{1}{N} \sum_{n=1}^N(W^Tx_n - y_n)^2 = \frac{1}{N} \sum_{n=1}^N(x_n^TW - y_n)^2 $$

(次方便显示将 向量W与向量X位置交换，因为是向量内积，符合交换律)

$$ = \frac{1}{N}  \left \| \begin{aligned} x_1^TW -y_1 \\ x_2^TW- y_2 \\ \cdots \\ x_N^TW - y_N \end{aligned} \right \| ^2 $$

（将平方和转换成矩阵平方的形式）

$$ = \frac{1}{N} \left \| \begin{bmatrix} -x_1^T- \\ -x_2^T- \\ \cdots \\ -x_N^T- \end{bmatrix}W -  \begin{bmatrix} y_1 \\ y_2 \\ \cdots \\ y_N \end{bmatrix}  \right \|^2 $$

（再拆解成矩阵X和向量w与向量y的形式）

$$ = \frac{1}{N} \left \| \underbrace{X^R}_{N \times (d+1)} \underbrace w_{(d+1) \times 1} - \underbrace y_{N \times 1} \right \|^2$$

再回到最初的目标寻找一个最小的$E_{in}(W)$ ， 如公式9-7所示：

$$\min_w E_{in}(W) = \min_w || X^T w - y||^2$$

求解此问题，需要了解左式，其唯一（d=1时）示意如图9-2所示

![Markdown](http://i1.bvimg.com/602813/68c73269610d328d.png)

<center>图9-2 一维示意图</center>

可以看出该函数为连续（continuous）、可微（differentiable）的凸（convex）函数，其中连续及可微的概念，学过高等数学的都应该有所了解，凸函数说的通俗点就如图9-2所示，像一个山谷一样的形式（注意国内数学教材中的凹函数是这里凸函数的定义，有点囧），寻找的最佳的$E_{in}(W)$便是山谷的最低点， 对应图中的黑点，以数学的形式表示即梯度（gradient）为0 的点。（我理解的梯度，大概意思是某一向量其各个分量的偏导数组成的向量），梯度为的表示方式如公式9-8所示。

$$\nabla_w E_{in}(W) = \begin{bmatrix} \frac{\partial E_{in}(w)}{\partial w_1} \\ \frac{\partial E_{in}(w)}{\partial w_2} \\ \vdots \\ \frac{\partial E_{in}(w)}{\partial w_d} \end{bmatrix} = \begin{bmatrix}0 \\ 0 \\ 0 \\ 0 \end{bmatrix}$$

其中 $\nabla$ 即梯度符号，需要寻找的是$W_{LIN}$，该向量满足$\nabla_w E_{in}(W_{LIN}) = 0$，这里$W_{LIN}$的下标表示线型linear的意思，紧接着的问题是如何求解$\nabla_w E_{in}(W_{LIN}) = 0$ 的$W_{LIN}。

继续对公式9-6做转化，如公式9-9所示。

$$E_{in}(w) = \frac{1}{N}|| X^Tw - y||^2 = \frac{1}{N}(w^T \underbrace{X^TX}_Aw - 2w^T \underbrace{X^Ty}_b + \underbrace{y^Ty}_c)$$

其中$X^TX$用矩阵A表示， $X^Ty$用向量b表示，$y^Ty$用标量c表示，紧接着对$E_{in}(w)求梯度。向量对向量求导，可能很多人都没有接触甚至没有听说过，最多也就是了解向量对某标量求导。可通过图9-3在w为标量情况下的对比形式，理解$E_{in}(w)$求梯度的步骤。

![Markdown](http://i1.bvimg.com/602813/2d02a315162836a5.png)

![Markdown](http://i1.bvimg.com/602813/4910ebe26cd5562f.png)

图9-3 a) w为标量时求$E_{in}(W)$的梯度，b)w为向量时$E_{in}(w)$的梯度

线性代数的美妙之处就在于此，如此的相似，因此$\nabla E_{in}(w)$ 可以写成公式9-10的形式

$$ \nabla_wE_{in}(W) = \frac{2}{N}(X^TXw - X^Ty)$$

另公式9-10求梯度结果为0，即使$E_{in}(w)$ 最小。在输入空间X与输出向量y都为已知的情况下，如何求解最佳的假设函数，$W_{LIN}$ 呢？求解该问题分为两种情况，一是在$X^TX$可逆的情况下，求解该问题很简单，将公式9-10右边的部分设为0，如公式9-11所示。

$$W_{LIN} = \underbrace{(X^TX)^{-1}X^T}_{X^{\uparrow}}y \quad \text{公式9-11}$$

其中$X^ \uparrow $ 表示矩阵X的伪逆（pseudo-inverse），注意此处输入矩阵X在很少的情况下才是方阵（N=d+1时）。而这种伪逆矩阵的形式和方阵中的逆矩阵具有很多相似的性质，因此才有此名称。还有一点需要说明，$X^TX$大部分的情况下是可逆的，原因是在进行机器学习时，通常满足$N \ge d+1$，即样本数量N远远大于样本的维度d加1，因此在$X^TX$中存在足够的自由度使其可以满足可逆的条件。

另一种是$X^TX$不可逆的情况，实际上可以得到许多满足条件的解，只需要通过其他的方式求解出$X^ \uparrow $，选择其中一个满足条件$W_{LIN} = X^ \uparrow  y $的解。

总结下线性回归算法的求解过程，首先通过已知的数据集，构建输入矩阵X与输出向量y，如公式9-12所示。

$$ X^T = \underbrace{\begin{bmatrix} -x_1^T- \\ -x_2^T- \\ \vdots \\ -x_N^T- \end{bmatrix}}_{N \times (d+1)} \qquad y= \underbrace{\begin{bmatrix}  y_1 \\ y_2 \\ \vdots \\ y_N\end{bmatrix}}_{N \times 1} $$

通过公式9-12直接求得违逆$\underbrace{X^{\uparrow}}_{(d+1)\times N}$

再通过公式9-11求得假设函数，如公式9-13所示

$$\underbrace{ W_{LIN}}_{(d+1)\times 1} = X^{\uparrow}y$$

##  9.3 Generalization Issue 泛化问题

本小节讨论的问题理解起来不简单，目前自己还是一知半解，如有表述不正确的地方还希望指正。

首先要回答一个问题，上一小节中使用到的求解最佳假设函数$W_{LIN}$的算法，是否能算是机器学习？

如回答不是，其理由很简单，求解只一步就完成了，不像前面章节中提到的学习方法需要很多步的过程。实际上，这种求解方式在数学中被称作解析解（analytical solution）或者叫封闭解或闭式解（closed-form solution），此种解是一些严格的公式，给出任意的自变量就可以求出其因变量，通常与数值解对应。因此这种求解方式并不像之前提到的PLA等算法时一步一步迭代求出的$E_{in}(W)$的最小解。

回答是的理由更看重结果，这种直接求解方式是数学推导中的精确解，因此求出的$W_{LIN}$一定是$E_{in}(w)$的最小解，符合求解条件，而且求解伪逆算法（此方法被称为高斯消元法，又见高斯，查了一下一共有110项以他名字命名的成果，整个机器学习笔记中你还会不断的听到以他命名的成果）并非如公式展示中显示的那样，一步就可以得出最终结果，而是需要几次的循环迭代（观察了矩阵求伪逆的程序，好像是三层循环，也就印证了NG在他机器学习课程中提到的矩阵求逆的复杂度为$O(N^3)$ ,只是被程序封装的看不出迭代的过程而已。而判断是否发生机器学习过程最主要标准是学习到的$E_{out}(w)$足够好！

其实通过改进VC限制，也可以证明在线性回归问题中VC起到了很好的约束作用，即找到了好的$E_{in}(W)$就可以保证$E_{out}(W)$还不错，这里不再证明，因为是件非常繁琐的过程。此处只需要记住VC限制不只在二元分类问题中起作用，在线性回归问题中也发挥着作用。

但是本节使用一种比VC限制更容易证明的保证，来说明解析解也可以得到一个好的$E_{out}(W)$

以下给出证明：为什么解析解求出的$E_{in}(W_{LIN})$的结果是好的。而有关的$E_{out}(W_{LIN})$证明与之类似。

















 



















