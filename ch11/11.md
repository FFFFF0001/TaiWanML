# 十一、Linear Models for Classification 用于分类的线性模型

## 11.1 Linear Models for Binary Classification   用于二元分类的线性模型

目前叙述的算法模型主要有3类：线性二元分类，线性回归，logistic回归，这三个模型的最主要的相同点在假设函数和错误函数中都出现了线性得分函数（linear scoring function），如公式11-1所示。

$s = w^Tx$

三类模型与得分s之间的关系如图11-1所示。

![Markdown](http://i4.bvimg.com/602813/d97737b51385624b.png)

图11-1 三类模型与得分s的关系

最左为线性二元分类，其假设函数为![img](http://images.cnitblog.com/blog/489652/201503/141033010899368.png)，一般使用0/1错误，通过![img](http://images.cnitblog.com/blog/489652/201503/141033015584913.png)求解最优权值向量w比较困难；中间为线性回归模型，其假设函数为![img](http://images.cnitblog.com/blog/489652/201503/141033019956512.png)，一般使用平方错误，可直接通过解析解求解最优w；最右为logistic回归模型，假设函数为![img](http://images.cnitblog.com/blog/489652/201503/141033027619614.png)，使用交叉熵错误，通过梯度下降法求出近似的w。

从上述分析不难看出，线性二元分类问题的求解方式最为困难，但与另外两种模型存在着共同点——得分s，能否利用这两种模型的算法近似求得二分类问题的最优w呢？

回顾10.2节，logistic回归的错误，可用符号![img](http://images.cnitblog.com/blog/489652/201503/141033031992214.png) 表示，其中CE为交叉熵（cross-entropy ）的缩写，可以写成公式11-2所示。

$$err_{CE} = err(w,x,y) = err_{CE}(s,y) = \ln(1+\exp(-yw^T)) = ln(1+\exp(-ys))$$

是否二元分类模型和线性回归模型的错误函数可以写成关于$ys$ 的形式？答案是可以的，如图11-2所示。

![Markdown](http://i4.bvimg.com/602813/9787a921a675457d.png)

图11-2 三类模型的错误函数

二元分类模型和线性回归模型错误函数中的转换都用到了$y \in \{+1, -1\}$ 的性质。接着观察三类模型的错误函数与ys之间的关系。本节开头回顾了s的物理意义为得分，此处ys的物理意义是正确的得分 ，因此ys越大越好，表示两者接近且同号。

根据图11-2中的三类模型的错误函数有关ys的公式，可以得出如图11-3所示的关系图。

![Markdown](http://i4.bvimg.com/602813/b338aa4f00a92dd9.png)

图11-3 三类模型的错误函数与ys的关系图

其中蓝色的折线表示0/1错误$err_{0/1}$ ，在ys大于0时，$err_{0/1}=0$，反之$err_{0/1}=1$；红色的抛物线表示平方错误$srr_{SQR}$，在 $ys \ll 1$时与$err_{0/1}$在该范围内所表现出的特征相似，但是在$ys \gg 1$时与在该范围内所表达的效果相去甚远，因此只有在$err_{SQR}$很小的情况下，可以使用$err_{SQR}$取代$err_{0/1}$；墨绿的曲线表示$err_{CE}$，同样如图11-3所示也只有在$err_{CE}$很小的情况下，$err_{CE}$和$err_{0/1}$可互相取代。但是$err_{CE}$跟想得到的错误曲线还有一些差距，因此略做转变，得到公式11-3。

$$err_{SCE} = log_2(1 + \exp(-ys))$$ 

其中$err_{SCE}$表示缩放的（scaled）$err_{CE}$，即对$srr_{CE}$做了一个换底，因此可以得到图11-4。

![Markdown](http://i1.bvimg.com/602813/606e9292d6340c73.png)

图11-4$err_{SCE}$关于ys的图

如图11-4中墨绿色的线表示$err_{SCE}$，从图中可以看出，该错误函数很适合做$err_{0/1}$的上限，在$err_{SCE}$很小的情况下，$err_{SCE}$和$err_{0/1}$可互相取代，如公式11-4所示。

$$err_{0/1}(s,y) \le err_{SCE}(s,y) = \frac{1}{\ln2} err_{CE}(s,y)$$

通过公式11-4可以得出$E_{in}^{0/1}$和$E_{out}^{0/1}$的上限，如公式11-5和公式11-6所示。

$$E_{in}^{0/1} \le E_{in}^{SCE}(w) = \frac{1}{\ln 2}E_{in}^{CE}(w)$$

$$E_{out}^{0/1} \le E_{out}^{SCE}(w) = \frac{1}{\ln 2}E_{out}^{CE}(w)$$

再通过VC限制理论可以得到公式11-7。

$$E_{out}^{0/1} \le E_{out}^{SCE}(w) + \Omega^{0/1}= \frac{1}{\ln 2}E_{out}^{CE}(w) + \Omega^{0/1}$$

第一个不等号连接的是在VC限制下$E_{out}^{0/1}(w)$和其上界，概念见7.4节, 其中$\Omega^{0/1}$函数是在7.4节中提到过的模型复杂度，在二元分类中可以写成的形$\sqrt{\frac{8}{N}\ln(\frac{4(2N)^{d_{VC}}}{\delta})}$式。

因此得到如下结论：小的$E_{out}^{0/1}(w)$可以通过小的$E_{in}^{CE}(w)$得出。同理可以证明小的$E_{out}^{0/1}(w)$也可以通过小的$E^{SQR}_{in}(w)$得出，即线性回归模型和logistic回归模型可以用作二元分类。

算法流程一般是在输出空间$y \in \{-1, +1\}$ 的情况下，通过线性回归和logistic回归相对应的求解方法求出最优 $w_{REG}$；

将求得的$w_{REG}$代入公式sign，得到最优假设函数 $g(x) = sign(w_{REG}^Tx)$。

三类模型做分类的利弊分析如表11-1所示。

![Markdown](http://i1.bvimg.com/602813/bd139a7dc18dc7c0.png)

线性回归一般只作为PLA、pocket、logistic回归的初始向量$w_0$ ；logistic回归经常取代pocket算法。

## 11.2 Stochastic Gradient Descent    随机梯度下降

如公式11-8为迭代优化算法的通式，学过的PLA的迭代算法如公式11-9，logistic回归中梯度下降的迭代公式如公式11-10。

$w_{t+1} = w_t + \eta \cdot v$



 