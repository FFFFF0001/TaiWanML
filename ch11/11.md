# 十一、Linear Models for Classification 用于分类的线性模型

## 11.1 Linear Models for Binary Classification   用于二元分类的线性模型

目前叙述的算法模型主要有3类：线性二元分类，线性回归，logistic回归，这三个模型的最主要的相同点在假设函数和错误函数中都出现了线性得分函数（linear scoring function），如公式11-1所示。

$s = w^Tx$

三类模型与得分s之间的关系如图11-1所示。

![Markdown](http://i4.bvimg.com/602813/d97737b51385624b.png)

图11-1 三类模型与得分s的关系

最左为线性二元分类，其假设函数为![img](http://images.cnitblog.com/blog/489652/201503/141033010899368.png)，一般使用0/1错误，通过![img](http://images.cnitblog.com/blog/489652/201503/141033015584913.png)求解最优权值向量w比较困难；中间为线性回归模型，其假设函数为![img](http://images.cnitblog.com/blog/489652/201503/141033019956512.png)，一般使用平方错误，可直接通过解析解求解最优w；最右为logistic回归模型，假设函数为![img](http://images.cnitblog.com/blog/489652/201503/141033027619614.png)，使用交叉熵错误，通过梯度下降法求出近似的w。

从上述分析不难看出，线性二元分类问题的求解方式最为困难，但与另外两种模型存在着共同点——得分s，能否利用这两种模型的算法近似求得二分类问题的最优w呢？

回顾10.2节，logistic回归的错误，可用符号![img](http://images.cnitblog.com/blog/489652/201503/141033031992214.png) 表示，其中CE为交叉熵（cross-entropy ）的缩写，可以写成公式11-2所示。

$$err_{CE} = err(w,x,y) = err_{CE}(s,y) = \ln(1+\exp(-yw^T)) = ln(1+\exp(-ys))$$

是否二元分类模型和线性回归模型的错误函数可以写成关于$ys$ 的形式？答案是可以的，如图11-2所示。

![Markdown](http://i4.bvimg.com/602813/9787a921a675457d.png)

图11-2 三类模型的错误函数

二元分类模型和线性回归模型错误函数中的转换都用到了$y \in \{+1, -1\}$ 的性质。接着观察三类模型的错误函数与ys之间的关系。本节开头回顾了s的物理意义为得分，此处ys的物理意义是正确的得分 ，因此ys越大越好，表示两者接近且同号。

根据图11-2中的三类模型的错误函数有关ys的公式，可以得出如图11-3所示的关系图。

![Markdown](http://i4.bvimg.com/602813/b338aa4f00a92dd9.png)

图11-3 三类模型的错误函数与ys的关系图

其中蓝色的折线表示0/1错误$err_{0/1}$ ，在ys大于0时，$err_{0/1}=0$，反之$err_{0/1}=1$；红色的抛物线表示平方错误$srr_{SQR}$，在 $ys \ll 1$时与$err_{0/1}$在该范围内所表现出的特征相似，但是在$ys \gg 1$时与在该范围内所表达的效果相去甚远，因此只有在$err_{SQR}$很小的情况下，可以使用$err_{SQR}$取代$err_{0/1}$；墨绿的曲线表示$err_{CE}$，同样如图11-3所示也只有在$err_{CE}$很小的情况下，$err_{CE}$和$err_{0/1}$可互相取代。但是$err_{CE}$跟想得到的错误曲线还有一些差距，因此略做转变，得到公式11-3。

$$err_{SCE} = log_2(1 + \exp(-ys))$$ 

其中$err_{SCE}$表示缩放的（scaled）$err_{CE}$，即对$srr_{CE}$做了一个换底，因此可以得到图11-4。

![Markdown](http://i1.bvimg.com/602813/606e9292d6340c73.png)

图11-4$err_{SCE}$关于ys的图

如图11-4中墨绿色的线表示$err_{SCE}$，从图中可以看出，该错误函数很适合做$err_{0/1}$的上限，在$err_{SCE}$很小的情况下，$err_{SCE}$和$err_{0/1}$可互相取代，如公式11-4所示。

$$err_{0/1}(s,y) \le err_{SCE}(s,y) = \frac{1}{\ln2} err_{CE}(s,y)$$

通过公式11-4可以得出$E_{in}^{0/1}$和$E_{out}^{0/1}$的上限，如公式11-5和公式11-6所示。

$$E_{in}^{0/1} \le E_{in}^{SCE}(w) = \frac{1}{\ln 2}E_{in}^{CE}(w)$$

$$E_{out}^{0/1} \le E_{out}^{SCE}(w) = \frac{1}{\ln 2}E_{out}^{CE}(w)$$

再通过VC限制理论可以得到公式11-7。

$$E_{out}^{0/1} \le E_{out}^{SCE}(w) + \Omega^{0/1}= \frac{1}{\ln 2}E_{out}^{CE}(w) + \Omega^{0/1}$$

第一个不等号连接的是在VC限制下$E_{out}^{0/1}(w)$和其上界，概念见7.4节, 其中$\Omega^{0/1}$函数是在7.4节中提到过的模型复杂度，在二元分类中可以写成的形$\sqrt{\frac{8}{N}\ln(\frac{4(2N)^{d_{VC}}}{\delta})}$式。

因此得到如下结论：小的$E_{out}^{0/1}(w)$可以通过小的$E_{in}^{CE}(w)$得出。同理可以证明小的$E_{out}^{0/1}(w)$也可以通过小的$E^{SQR}_{in}(w)$得出，即线性回归模型和logistic回归模型可以用作二元分类。

算法流程一般是在输出空间$y \in \{-1, +1\}$ 的情况下，通过线性回归和logistic回归相对应的求解方法求出最优 $w_{REG}$；

将求得的$w_{REG}$代入公式sign，得到最优假设函数 $g(x) = sign(w_{REG}^Tx)$。

三类模型做分类的利弊分析如表11-1所示。

![Markdown](http://i1.bvimg.com/602813/bd139a7dc18dc7c0.png)

线性回归一般只作为PLA、pocket、logistic回归的初始向量$w_0$ ；logistic回归经常取代pocket算法。

## 11.2 Stochastic Gradient Descent    随机梯度下降

如公式11-8为迭代优化算法的通式，学过的PLA的迭代算法如公式11-9，logistic回归中梯度下降的迭代公式如公式11-10。

$w_{t+1} = w_t + \eta \cdot v$

$$w_{t+1} = w_t + y_{n(t)} \cdot x_{n(t)}$$

$$w_{t+1} = w_t - \eta \nabla E_{in}(w_t) = w_t - \eta(\frac{1}{N} \sum_{n=1}^{N}\theta(-y_nw_t^Tx_n)(-y_nx_n)) = w_t + \eta(\frac{1}{N} \sum_{n=1}^N \theta(-y_nw_t^Tx_n)(y_nx_n)$$

对比以上两种迭代优化方法：PLA与logistic回归的梯度下降。发现PLA只需要通过一个样本点便可计算出$w_{t+1}$，即每次迭代的时间复杂度为 $O(1)$；logistic回归的梯度下降需要遍历所有的样本点才能计算出$w_{t+1}$，即每次迭代的时间复杂度为$O(n)$。有无可能将logistic回归每次迭代时间复杂度降为$O(1)$？

观察公式11-10，方向向量v，v≈$-\nabla E_{in}(w_t)$，该梯度是通过所有的样本点加权求和再取平均得到的，如何使用一个样本点的取值近似整体的平均值？

可以将求平均的过程理解为求期望值，此处使用在N个样本中随机抽取一个样本点求出的梯度取代原来的期望梯度，这种随机选取的梯度称为随机梯度（stochastic gradient），可用符号$\nabla _w err(w, x, y)$表示，而真实的梯度与随机梯度的关系如公式11-11。

$$\nabla E_{in}(w_t) = \varepsilon_{random \quad n} \nabla _w err(w,x_n,y_n) $$

随机梯度值可以看做真实的梯度值加上一个噪音，使用随机梯度取代真实梯度做梯度下降的算法称作随机梯度下降（stochastic gradient descent），简称SGD。这种替代的理论基础是在迭代次数足够多的情况下，平均的随机梯度和平均的真实梯度相差不大。

该算法的优点是简单，容易计算，适用于大数据或者流式数据；缺点是不稳定。

Logistic回归的随机梯度下降的迭代如公式11-12所示。

$w_{t+1} = w_t + \eta \underbrace{(-y_nw_t^Tx_n)(y_nx_n)}_{-\nabla_w err(w,x_n,y_n)}$

是否联想到了其他的迭代算法？PLA，如公式11-13所示。

$$w_{t+1} = w_t + \underbrace{1}_\eta \underbrace{[|sign(w_t^Tx_n) \ne y_n|]y_nx_n}_{v}$$

因此logistic回归随机梯度下降类似于"软"的PLA，为什么称为软的？原因是它的$y_nx_n$之前的权值并没有那么绝对不是1就是0，而是一个在0~1之间的值。在公式11-12中，如果$\eta =1$且$w_t^Tx_n$始终是一个很大的值，则logistic回归随机梯度下降相当于是PLA。

## 11.3 Multiclass via Logistic Regression    通过logistic回归实现多类别分类

多类别分类有许多应用场景，特别是在识别（recognition）领域。

 